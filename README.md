# Data Team / Exercice technique

> ğŸ‘‹ Bienvenue Ã  toi ! Tu vas trouver ici tout le contexte dont tu as besoin pour rÃ©aliser le test technique qui fait parti du processus de recrutement chez Jump. 

## PrÃ©sentation

### La Data Platform
Ce dÃ©pÃ´t contient l'ensemble du code qui constitue la Data Platform. La Data Platform constitue **l'ensemble des moyens techniques mis en oeuvre pour rÃ©pondre aux besoins qui nÃ©cessitent l'exploitation de la donnÃ©e chez Jump** (rapports et dashboards, extractions ad-hoc, etc.) 

La clÃ© de voute de la Data Platform est le Lakehouse : c'est une base de donnÃ©e analytique qui est composÃ©e de 4 couches :
* La couche Sources qui contient une copie de **l'ensemble des donnÃ©es brutes** issue des applications utilisÃ©es chez Jump (l'application dÃ©velopÃ©e par les Ã©quipes Backend et Frontend, le CRM administrÃ© par l'Ã©quipe Sales, etc.)
* La couche Staging qui est trÃ¨s similaire Ã  la couche prÃ©cÃ©dente mais contient **quelques Ã©tapes de nettoyage, de filtrage**, etc. ;
* La couche Intermediate qui s'appuie sur la couche prÃ©cÃ©dente et contient **un modÃ¨le homogÃ¨ne et cohÃ©rent qui couvre l'ensemble du pÃ©rimÃ¨tre fonctionnel** adressÃ© par Jump (facturation, CDI des salariÃ©s portÃ©s, etc.) ;
* Le couche Marts qui contient **des modÃ¨les plus complexes mais Ã  forte valeur ajoutÃ©e** nÃ©cessaires pour rÃ©pondre Ã  des besoins fonctionnels plus poussÃ©s. 

![Architecture](docs/architecture.png)


> ğŸ’¡ Bien que ce dÃ©pÃ´t existe que dans le cadre du test technique, l'architecture que l'on a en interne est trÃ¨s semblable Ã  celle dÃ©crite ici (mÃªme s'il y a plus que 2 applications Ã  intÃ©grer que les problÃ©matiques sont beaucoup plus larges).


### Le modÃ¨le de donnÃ©es

Le business de Jump est assez simple : 
* Un CDI chez Jump est materialisÃ© par :
    * Une date de dÃ©but,
    * Une date de fin,
    * Une entitÃ© (qui peut Ãªtre `blue` si le freelance souscri Ã  Jump Blue ou `green` s'il souscrit Ã  Jump Green) ;
* Un freelance peut avoir plusieurs CDI (Ã  condition que les dates entre deux CDI ne se chevauchent pas) ;
* Dans le cadre de son CDI, un freelance peut facturer un client.

![ModÃ¨le de donnÃ©es](docs/data-model.png)


## DÃ©tails technique

### Composants

La Data Platform s'appuie sur les technologies suivantes : 
* Le Lakehouse est **une base de donnÃ©es [DuckDB](https://duckdb.org/)** :
    * Le fichier se trouve dans `./data/lakehouse/lakehouse.duckdb`, 
    * Chacune des couches logiques citÃ©es ci-dessus est un schÃ©ma ;
* L'ensemble des transformations est rÃ©alisÃ© **Ã  l'aide de [DBT](https://www.getdbt.com/)** grÃ¢ce Ã  [ce projet](./dbt/) ;
* Une [CLI](./cli) en Python qui orchestre les diffÃ©rentes Ã©tapes d'alimentation de la Data Platform :
    * L'Ã©tape `extract` pour extraire les donnÃ©es depuis les applications sous forme de fichiers CSV dans le dossier `./data/sources`, 
    * L'Ã©tape `load` pour charger les donnÃ©es extraites dans le schÃ©ma `sources` du Lakehouse (qui va aller se trouver dans `./data/lakehouse`), 
    * L'Ã©tape `transform` pour alimenter les schÃ©mas `staging`, `intermediate` et `marts`.

> âš ï¸ Naturellement, dans le cadre du test technique, on extrait les donnÃ©es d'aucune application. Elles sont en rÃ©alitÃ© gÃ©nÃ©rÃ©es alÃ©atoirement par la CLI ([ici](./cli/src/jump/data_platform/sources/app/app.py) et [lÃ ](./cli/src/jump/data_platform/sources/crm/crm.py)).



### Utilisation

### TL;DR...

Pour pouvoir lancer les commandes, tu auras besoin de Docker et de Make. 

Pour construire l'image Docker qui contient la Data Platform et lancer une chaine d'alimentation complÃ¨te, tu as juste Ã  lancer la commande `make`. 

> ğŸ’¥ La commande `make` seule doit fonctionner sans aucune erreur... Si tu rencontre la moindre erreur, contacte-nous : cela ne devrait pas arriver !

Si tu veux plus de dÃ©tails sur les targets disponibles, tu peux lancer `make help`.


### Et en plus long ? 

La conteneurisation est faite Ã  l'aide de Docker et de [ce Dockerfile](./docker/Dockerfile) et toutes les commandes pour builder sont embarquÃ©es dans [ce Makefile](./Makefile).

 Les targets suivantes sont disponibles :
* `make build` : construit l'image Docker qui embarque la CLI, le projet DBT, etc.
* `make extract` : lance l'extract des donnÃ©es de l'application et du CRM
* `make load` : lance l'inÃ©gration des extractions dans le schÃ©ma `source` du Lakehouse
* `make transform` : transforme les donnÃ©es et alimente les schÃ©mas `staging`, `intermediate` et `marts` (Ã  l'aide du [projet DBT](./dbt/))
* `make query` : lance le REPL de DuckDB pour requÃªter le lakehouse

> â“ Tout est clair ? Si oui, retrouve [ici](./exercices/positions/data-analyst.md) les exercices Ã  rÃ©aliser ! 
