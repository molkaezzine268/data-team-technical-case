# Data Platform

> ğŸ‘‹ Bienvenue Ã  toi ! Tu vas trouver ici tout le contexte dont tu as besoin pour rÃ©aliser le test technique qui fait parti du processus de recrutement chez Jump. 

## PrÃ©sentation globale

Ce dÃ©pÃ´t contient l'ensemble du code qui constitue la Data Platform. La Data Platform constitue **l'ensemble des moyens techniques mis en oeuvre pour rÃ©pondre aux besoins qui nÃ©cessitent l'exploitation de la donnÃ©e chez Jump** (rapports et dashboards, extractions ad-hoc, etc.) 

La clÃ© de voute de la Data Platform est le Lakehouse : c'est une base de donnÃ©e analytique qui est composÃ©e de 4 couches :
* La couche Sources qui contient une copie de **l'ensemble des donnÃ©es brutes** issue des applications utilisÃ©es chez Jump (l'application dÃ©velopÃ©e par les Ã©quipes Backend et Frontend, le CRM administrÃ© par l'Ã©quipe Sales, etc.)
* La couche Staging qui est trÃ¨s similaire Ã  la couche prÃ©cÃ©dente mais contient **quelques Ã©tapes de nettoyage, de filtrage**, etc. ;
* La couche Intermediate qui s'appuie sur la couche prÃ©cÃ©dente et contient **un modÃ¨le homogÃ¨ne et cohÃ©rent qui couvre l'ensemble du pÃ©rimÃ¨tre fonctionnel** adressÃ© par Jump (facturation, CDI des salariÃ©s portÃ©s, etc.) ;
* Le couche Marts qui contient **des modÃ¨les plus complexes mais Ã  forte valeur ajoutÃ©e** nÃ©cessaires pour rÃ©pondre Ã  des besoins fonctionnels plus poussÃ©s. 

![SchÃ©ma](docs/schema.png)


> ğŸ’¡ Bien que ce dÃ©pÃ´t existe que dans le cadre du test technique, l'architecture que l'on a en interne est trÃ¨s semblable Ã  celle dÃ©crite ici (mÃªme s'il y a plus que 2 applications Ã  intÃ©grer que les problÃ©matiques sont beaucoup plus larges).


## ImplÃ©mentation technique

La Data Platform s'appuie sur les technologies suivantes : 
* Le Lakehouse est **une base de donnÃ©es [DuckDB](https://duckdb.org/)** :
    * Le fichier se trouve dans `./data/lakehouse/lakehouse.duckdb`, 
    * Chacune des couches logiques citÃ©es ci-dessus est un schÃ©ma ;
* L'ensemble des transformations est rÃ©alisÃ© **Ã  l'aide de [DBT](https://www.getdbt.com/)** grÃ¢ce Ã  [ce projet](./dbt/) ;
* Une [CLI](./cli) en Python (nommÃ©e `data-platform`) qui orchestre les diffÃ©rentes Ã©tapes d'alimentation de la Data Platform :
    * L'Ã©tape `extract` pour copier les donnÃ©es depuis les applications sous forme de base de donnÃ©es [SQLite](https://www.sqlite.org/index.html) dans le dossier `./data/sources`, 
    * L'Ã©tape `load` pour charger les donnÃ©es extraites dans le schÃ©ma `sources` du Lakehouse (qui va aller se trouver dans `./data/lakehouse`), 
    * L'Ã©tape `transform` pour alimenter les schÃ©mas `staging`, `intermediate` et `marts`.

> âš ï¸ En rÃ©alitÃ©, on extrait les donnÃ©es d'aucune application. Elles sont en rÃ©alitÃ© gÃ©nÃ©rÃ©es alÃ©atoirement par la CLI ([ici](./cli/src/jump/data_platform/sources/app/app.py) et [lÃ ](./cli/src/jump/data_platform/sources/crm/crm.py)).

L'ensemble est conteneurisÃ© Ã  l'aide de Docker et de [ce Dockerfile](./docker/Dockerfile).


## Utilisation

> ğŸ—ï¸ Pour pouvoir lancer les commandes, tu auras besoin de Docker et de Make. 

* `make build` : construit l'image Docker qui embarque la CLI, le projet DBT, etc.
* `make extract` : lance l'extract des donnÃ©es de l'application et du CRM
* `make load` : lance l'inÃ©gration des extractions dans le schÃ©ma `source` du Lakehouse
* `make transform` : transforme les donnÃ©es et alimente les schÃ©mas `staging`, `intermediate` et `bronze` (Ã  l'aide du [projet DBT](./dbt/))