> üá¨üáß You'll find the english version [here](./README_en.md)! 

# Data Team / Exercice technique

> üëã Bienvenue √† toi ! Tu vas trouver ici tout le contexte dont tu as besoin pour r√©aliser le test technique qui fait parti du processus de recrutement chez Jump. 

## Pr√©sentation

### La Data Platform
Ce d√©p√¥t contient l'ensemble du code qui constitue la Data Platform. La Data Platform constitue **l'ensemble des moyens techniques mis en oeuvre pour r√©pondre aux besoins qui n√©cessitent l'exploitation de la donn√©e chez Jump** (rapports et dashboards, extractions ad-hoc, etc.) 

La cl√© de voute de la Data Platform est le Lakehouse : c'est une base de donn√©e analytique qui est compos√©e de 4 couches :
* La couche Sources qui contient une copie de **l'ensemble des donn√©es brutes** issue des applications utilis√©es chez Jump (l'application d√©velop√©e par les √©quipes Backend et Frontend, le CRM administr√© par l'√©quipe Sales, etc.)
* La couche Staging qui est tr√®s similaire √† la couche pr√©c√©dente mais contient **quelques √©tapes de nettoyage, de filtrage**, etc. ;
* La couche Intermediate qui s'appuie sur la couche pr√©c√©dente et contient **un mod√®le homog√®ne et coh√©rent qui couvre l'ensemble du p√©rim√®tre fonctionnel** adress√© par Jump (facturation, CDI des salari√©s port√©s, etc.) ;
* Le couche Marts qui contient **des mod√®les plus complexes mais √† forte valeur ajout√©e** n√©cessaires pour r√©pondre √† des besoins fonctionnels plus pouss√©s. 

![Architecture](docs/architecture_fr.png)


> üí° Bien que ce d√©p√¥t existe que dans le cadre du test technique, l'architecture que l'on a en interne est tr√®s semblable √† celle d√©crite ici (m√™me s'il y a plus que 2 applications √† int√©grer que les probl√©matiques sont beaucoup plus larges).


### Le mod√®le de donn√©es

Le business de Jump est assez simple : 
* Un CDI chez Jump est materialis√© par :
    * Une date de d√©but,
    * Une date de fin,
    * Une entit√© (qui peut √™tre `blue` si le freelance souscrit √† Jump Blue ou `green` s'il souscrit √† Jump Green) ;
* Un freelance peut avoir plusieurs CDI (√† condition que les dates entre deux CDI ne se chevauchent pas) ;
* Dans le cadre de son CDI, un freelance peut facturer un client.

![Mod√®le de donn√©es](docs/data-model.png)


## D√©tails technique

### Composants

La Data Platform s'appuie sur les technologies suivantes : 
* Le Lakehouse est **une base de donn√©es [DuckDB](https://duckdb.org/)** :
    * Le fichier se trouve dans `./data/lakehouse/lakehouse.duckdb`, 
    * Chacune des couches logiques cit√©es ci-dessus est un sch√©ma ;
* L'ensemble des transformations est r√©alis√© **√† l'aide de [DBT](https://www.getdbt.com/)** gr√¢ce √† [ce projet](./dbt/) ;
* Une [CLI](./cli) en Python qui orchestre les diff√©rentes √©tapes d'alimentation de la Data Platform :
    * L'√©tape `extract` pour extraire les donn√©es depuis les applications sous forme de fichiers CSV dans le dossier `./data/sources`, 
    * L'√©tape `load` pour charger les donn√©es extraites dans le sch√©ma `sources` du Lakehouse (qui va aller se trouver dans `./data/lakehouse`), 
    * L'√©tape `transform` pour alimenter les sch√©mas `staging`, `intermediate` et `marts`.

> ‚ö†Ô∏è Naturellement, dans le cadre du test technique, on extrait les donn√©es d'aucune application. Elles sont en r√©alit√© g√©n√©r√©es al√©atoirement par la CLI ([ici](./cli/src/jump/data_platform/sources/app/app.py) et [l√†](./cli/src/jump/data_platform/sources/crm/crm.py)).



### Utilisation

### TL;DR...

Pour pouvoir lancer les commandes, tu auras besoin de Docker et de Make. 

Pour construire l'image Docker qui contient la Data Platform et lancer une chaine d'alimentation compl√®te, tu as juste √† lancer la commande `make` (sans argument). 

> üí• La commande `make` seule doit fonctionner sans aucune erreur... Si tu rencontre la moindre erreur, contacte-nous : cela ne devrait pas arriver !

Si tu veux plus de d√©tails sur les targets disponibles, tu peux lancer `make help`.


### Et en plus long ? 

La conteneurisation est faite √† l'aide de Docker et de [ce Dockerfile](./docker/Dockerfile) et toutes les commandes pour builder sont embarqu√©es dans [ce Makefile](./Makefile).

 Les targets suivantes sont disponibles :
* `make build` : construit l'image Docker qui embarque la CLI, le projet DBT, etc.
* `make extract` : lance l'extract des donn√©es de l'application et du CRM
* `make load` : lance l'in√©gration des extractions dans le sch√©ma `source` du Lakehouse
* `make transform` : transforme les donn√©es et alimente les sch√©mas `staging`, `intermediate` et `marts` (√† l'aide du [projet DBT](./dbt/))
* `make query` : lance le REPL de DuckDB pour requ√™ter le lakehouse

> N'h√©site pas √† explorer les donn√©es en parcourant le [projet DBT](./dbt/) et en requ√™tant les tables avec `make query`.

> ‚ùì Tout est clair ? Si oui, retrouve [ici](./exercices/positions/data-analyst_fr.md) les exercices √† r√©aliser ! 
